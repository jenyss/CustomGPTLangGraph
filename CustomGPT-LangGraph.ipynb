{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb50aa8-40a3-4e44-97ab-297dda41502e",
   "metadata": {},
   "source": [
    "<h1>Custom GPT based on LangGraph with Search and Summarization Tools</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580c729-398f-40cf-b61a-1f9a54aa6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_experimental PyMuPDF langchain langgraph chromadb openai python-dotenv langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a62dfe-ceab-46e2-8a96-04a7cd9a9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF extraction\n",
    "# import pdfplumber\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader, DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.tools import Tool\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import time\n",
    "from uuid import uuid4  # Ensure unique IDs\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8691a5-f109-4bd9-986e-86e8857767b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully reinitialized ChromaDB with a fresh 'company_name_docs' collection.\n"
     ]
    }
   ],
   "source": [
    "# Set Chroma persistence directory\n",
    "persist_directory = \"./chroma_company_name_db\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "try:\n",
    "    persistent_client = chromadb.PersistentClient(path=persist_directory, settings=Settings(allow_reset=True))\n",
    "    collections = persistent_client.list_collections()\n",
    "    collection_names = [coll.name for coll in collections]\n",
    "\n",
    "    if \"company_name_docs\" in collection_names:\n",
    "        persistent_client.delete_collection(\"company_name_docs\")\n",
    "        print(\"üóëÔ∏è Deleted existing ChromaDB collection: company_name_docs\")\n",
    "\n",
    "    time.sleep(1)\n",
    "    vector_db = persistent_client.get_or_create_collection(\"company_name_docs\")\n",
    "    print(\"‚úÖ Successfully reinitialized ChromaDB with a fresh 'company_name_docs' collection.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during ChromaDB initialization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9945ad-43bb-477c-8fe3-f7829e1157f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vector_db = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"company_name_docs\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# Define recursive text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Define semantic text splitter\n",
    "# splitter = SemanticChunker(\n",
    "#     embeddings=OpenAIEmbeddings(),\n",
    "#     breakpoint_threshold_type=\"percentile\",  # Method to determine breakpoints\n",
    "#     breakpoint_threshold_amount=85.0  # Threshold value\n",
    "# )\n",
    "\n",
    "\n",
    "### === Document Splitting Functions === ###\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF while preserving page numbers.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return [(page.get_text(\"text\"), pdf_path, page.number + 1) for page in doc]\n",
    "\n",
    "\n",
    "def load_text_file(txt_path):\n",
    "    \"\"\"Loads plain text files as single chunks for splitting.\"\"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "    return [(content, txt_path, 1)]\n",
    "\n",
    "\n",
    "def load_word_file(doc_path):\n",
    "    \"\"\"Loads Word documents using UnstructuredWordDocumentLoader in elements mode.\"\"\"\n",
    "    loader = UnstructuredWordDocumentLoader(doc_path, mode=\"elements\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Extract text and metadata from each element\n",
    "    return [(doc.page_content, doc_path, doc.metadata.get(\"page_number\", 1)) for doc in docs]\n",
    "\n",
    "\n",
    "def load_excel_file(excel_path):\n",
    "    \"\"\"Loads Excel files using DataFrameLoader, treating each row as a document.\"\"\"\n",
    "    df = pd.read_excel(excel_path)\n",
    "\n",
    "    # Ensure there is data\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è No data found in {excel_path}. Skipping.\")\n",
    "        return []\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    print(f\"\\nüìÇ Loading Excel File: {excel_path}\")\n",
    "    print(f\"üî¢ Total Rows: {len(df)}\\n\")\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Convert row to string (concatenating all columns)\n",
    "        row_content = \" | \".join(map(str, row.values))  # Join all values in the row\n",
    "        documents.append((row_content, excel_path, index + 1))  # Using index+1 as row ID\n",
    "\n",
    "        # Debug Print\n",
    "        print(f\"üìù Row {index + 1}: {row_content}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully processed {len(documents)} rows from {excel_path}.\\n\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def process_document(file_path):\n",
    "    \"\"\"Determines the correct processing method based on file type.\"\"\"\n",
    "    ext = os.path.splitext(file_path)[-1].lower()\n",
    "    \n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        return load_text_file(file_path)\n",
    "    elif ext in [\".docx\", \".doc\"]:\n",
    "        return load_word_file(file_path)\n",
    "    elif ext in [\".xlsx\", \".xls\"]:\n",
    "        return load_excel_file(file_path)\n",
    "    else:\n",
    "        print(f\"‚ùå Unsupported file type: {ext}. Skipping {file_path}.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "### === Document Processing and Embedding === ###\n",
    "\n",
    "def process_and_embed_documents(file_paths):\n",
    "    \"\"\"Processes and embeds documents one by one to avoid performance issues.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"üìÑ Processing: {file_path}\")\n",
    "        \n",
    "        # Extract text based on file type\n",
    "        doc_chunks = process_document(file_path)\n",
    "        \n",
    "        # Split text into chunks\n",
    "        for text, source, page in doc_chunks:\n",
    "            for chunk in splitter.split_text(text):\n",
    "                documents.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\"source\": source, \"page\": page}\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    # Assign unique IDs and add to vector DB\n",
    "    if documents:\n",
    "        uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "        vector_db.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "        # Debug Document Distribution\n",
    "        print(\"\\n=== START :: Document Distribution Analysis ================================================\\n\")\n",
    "        docs = vector_db._collection.get()\n",
    "        source_stats = {}\n",
    "        \n",
    "        # Count documents per source and page\n",
    "        for meta in docs[\"metadatas\"]:\n",
    "            source = meta[\"source\"]\n",
    "            page = meta[\"page\"]\n",
    "            \n",
    "            if source not in source_stats:\n",
    "                source_stats[source] = {\"total\": 0, \"pages\": set()}\n",
    "            \n",
    "            source_stats[source][\"total\"] += 1\n",
    "            source_stats[source][\"pages\"].add(page)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nDocument counts per source:\")\n",
    "        for source, stats in source_stats.items():\n",
    "            print(f\"\\nüìÅ {source}\")\n",
    "            print(f\"  Total chunks: {stats['total']}\")\n",
    "            print(f\"  Pages/Sections: {sorted(list(stats['pages']))}\")\n",
    "        \n",
    "        print(\"\\n=== END :: Document Distribution Analysis ==============================================\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ Successfully embedded {len(documents)} chunks.\")\n",
    "\n",
    "\n",
    "### === Search and Retrieval TOOLD with Citations === ###\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    \"\"\"Represents a citation from a retrieved document.\"\"\"\n",
    "    source: str = Field(..., description=\"The document name or source.\")\n",
    "    page: int = Field(..., description=\"The page number of the cited document.\")\n",
    "\n",
    "class CitedAnswer(BaseModel):\n",
    "    \"\"\"Structured response with citations.\"\"\"\n",
    "    answer: str = Field(..., description=\"The answer to the user's question.\")\n",
    "    citations: List[Citation] = Field(..., description=\"List of citations that justify the answer.\")\n",
    "\n",
    "\n",
    "def search_docs(query):\n",
    "    \"\"\"Performs vector search with citation retrieval and returns structured CitedAnswer.\"\"\"\n",
    "    print(f\"\\nüîç ############################################## search_docs tool called!\")\n",
    "\n",
    "    docs = vector_db.similarity_search_with_score(query, k=10)\n",
    "    if not docs:\n",
    "        print(\"‚ùå No documents found in search\")\n",
    "        return CitedAnswer(answer=\"No relevant data found.\", citations=[])\n",
    "\n",
    "    results = []\n",
    "    citations = []\n",
    "    snippets = []  # NEW: Stores only the most relevant snippets\n",
    "\n",
    "    for index, (doc, score) in enumerate(docs):\n",
    "        metadata = doc.metadata\n",
    "        source = metadata.get(\"source\", \"Unknown Source\")\n",
    "        page = metadata.get(\"page\", \"Unknown Page\")\n",
    "\n",
    "        # Extract only the part of text that directly matches the query\n",
    "        best_snippet = extract_relevant_snippet(doc.page_content, query)\n",
    "        snippets.append(best_snippet)\n",
    "\n",
    "        citations.append(Citation(source=source, page=page))\n",
    "\n",
    "    return CitedAnswer(answer=\"\\n\\n\".join(snippets), citations=citations)\n",
    "\n",
    "# NEW: Extract only the most relevant part of the retrieved text\n",
    "def extract_relevant_snippet(text, query):\n",
    "    \"\"\"Extracts the most relevant snippet from the retrieved text that matches the query.\"\"\"\n",
    "    sentences = text.split(\". \")\n",
    "    for sentence in sentences:\n",
    "        if query.lower() in sentence.lower():\n",
    "            return sentence + \".\"\n",
    "    return text[:300] + \"...\"  # Return first 300 chars if no clear match\n",
    "\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"SearchDocsTool\",\n",
    "    func=search_docs,\n",
    "    description=\"Must always be used to retrieve relevant documentation with citations. Searches product, API, and scripting documentation for relevant details with proper citations.\"\n",
    ")\n",
    "\n",
    "\n",
    "def summarization_tool(query):\n",
    "    #     \"\"\"\n",
    "    #     Retrieves relevant document content and summarizes it using the specified summarization technique.\n",
    "    #     Ensures all generated summaries include explicit citations.\n",
    "    #     Summarization Options: \"stuff\", \"map_reduce\", \"refine\". You can switch between these depending on your need!\n",
    "    #\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nüîç ############################################## summarization_tool tool called!\")\n",
    "\n",
    "        # Retrieve document content with citations and formatted doc input\n",
    "        cited_answer = search_docs(query)\n",
    "\n",
    "        if not cited_answer.answer.strip():\n",
    "            return CitedAnswer(answer=\"No relevant content found for summarization.\", citations=[])\n",
    "\n",
    "        summarization_type = \"map_reduce\"\n",
    "\n",
    "        if 'load_summarize_chain' not in globals():\n",
    "            return CitedAnswer(answer=\"Summarization functionality is not available.\", citations=[])\n",
    "\n",
    "        summarization_chain = load_summarize_chain(llm, chain_type=summarization_type)\n",
    "\n",
    "        # Pass ONLY the retrieved snippets to avoid LLM making up information\n",
    "        response = summarization_chain.invoke([Document(page_content=cited_answer.answer)])\n",
    "\n",
    "        return CitedAnswer(answer=response, citations=cited_answer.citations)\n",
    "\n",
    "    except Exception as e:\n",
    "        return CitedAnswer(answer=f\"Error during summarization: {str(e)}\", citations=[])\n",
    "\n",
    "\n",
    "summarization_tool = Tool(\n",
    "    name=\"SummarizationTool\",\n",
    "    func=summarization_tool,\n",
    "    description=\"Provides a high-level summary of document content with structured citations.\"\n",
    ")\n",
    "\n",
    "\n",
    "### === Define LangGraph Agent === ###\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "tools = [search_tool, summarization_tool]\n",
    "\n",
    "\n",
    "#WORKING\n",
    "# system_prompt = \"\"\"\n",
    "# You are a company-wide GPT assistant, available to answer:\n",
    "# - General product questions\n",
    "# - Detailed feature explanations\n",
    "# - Technical guidance on REST APIs, GraphQL APIs, and Company_name Script Language.\n",
    "\n",
    "# You have access to two tools:\n",
    "#  - \n",
    "# SearchDocsTool\n",
    "#  Retrieves relevant documentation with citations.\n",
    "#  - \n",
    "# SummarizationTool\n",
    "#  Provide a high-level summary with citations of the document(s) relevant to the query.\n",
    "\n",
    "# ### **Behavior Guidelines:**\n",
    "# 1. If the user asks a general question, answer directly.\n",
    "# 2. For queries about API details or scripting:\n",
    "#    - Retrieve relevant details using SearchDocsTool.\n",
    "#    - Use the retrieved information to answer the question.\n",
    "#    - Generate a structured response with citations, including document name and page number.\n",
    "#    - Provide code snippets when applicable.\n",
    "# 3. If SearchDocsTool does not return relevant data, then **DO NOT** invent examples or generate completions which are not a result from \n",
    "# SearchDocsTool\n",
    "#  tool. Only politely inform the user that you cannot provide an answer for the question.\n",
    "# 4. Ensure responses are precise and well-structured.\n",
    "# \"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a company-wide GPT assistant, available to answer:\n",
    "- General Company_name product questions\n",
    "- Detailed feature explanations\n",
    "- Technical guidance on REST APIs, GraphQL APIs, and Company_name Script Language.\n",
    "\n",
    "You have access to two tools:\n",
    "- SearchDocsTool: Retrieves relevant documentation with citations.\n",
    "- SummarizationTool: Provides a high-level summary with citations of the document(s) relevant to the query.\n",
    "\n",
    "### **Behavior Guidelines:**\n",
    "1. IMPORTANT: Your answers must strictly come from retrieved documents! Do NOT generated examples which are not retrieved from the provided documents!\n",
    "2. For specific product features or technical details:\n",
    "   - First use SearchDocsTool to retrieve relevant information\n",
    "   - Use the retrieved information to answer the question\n",
    "   - Include proper citations (document name and page number)\n",
    "   - Provide code snippets when applicable\n",
    "3. Use SummarizationTool only when a broad overview of multiple documents would better serve the query.\n",
    "4. IMPORTANT: If tools do not return relevant data, DO NOT fabricate answers. Instead, clearly state: \"I don't have sufficient documentation to answer this question accurately.\"\n",
    "5. Ensure all responses are precise and well-structured.\n",
    "\"\"\"\n",
    "\n",
    "# Another Working SUGGESTION\n",
    "# system_prompt = \"\"\"\n",
    "# You are a company-wide GPT assistant, available to answer:\n",
    "# - General Company_name product questions\n",
    "# - Technical guidance on REST APIs, GraphQL APIs, and Company_name Script Language.\n",
    "\n",
    "# You have access to two tools:\n",
    "# - SearchDocsTool: Retrieves relevant documentation with citations.\n",
    "# - SummarizationTool: Provides a high-level summary with citations.\n",
    "\n",
    "# ### **Behavior Guidelines:**\n",
    "# 1. **IMPORTANT: Your answers must strictly come from retrieved documents. DO NOT invent or modify code examples.**\n",
    "# 2. For API details or technical questions:\n",
    "#    - Use SearchDocsTool first to retrieve relevant data.\n",
    "#    - If needed, use SummarizationTool.\n",
    "#    - **Include exact document citations for all statements.**\n",
    "# 3. If no relevant data is found, explicitly state:  \n",
    "#    - _\"I don't have sufficient documentation to answer this question accurately.\"_\n",
    "# 4. **If providing an example, ensure it is verbatim from the retrieved document. Do not paraphrase.**\n",
    "# \"\"\"\n",
    "     \n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    prompt=system_prompt,\n",
    "    response_format=CitedAnswer,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "### === Query Execution with Citations === ###\n",
    "\n",
    "def ask_question(question):\n",
    "    \"\"\"Executes a query against the agent.\"\"\"\n",
    "    inputs = {\"messages\": [(\"user\", question)]}\n",
    "    for step in agent.stream(inputs, stream_mode=\"values\"):\n",
    "        message = step[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "\n",
    "### === Example Usage === ###\n",
    "\n",
    "# List of files to process\n",
    "\n",
    "# ‚ùó‚ùó‚ùó List your document(s) paths here!\n",
    "file_list = [\n",
    "    \"example.pdf\",\n",
    "    \"Examples.txt\",\n",
    "    # \"sample-doc.docx\",\n",
    "    # \"test.xlsx\"\n",
    "]\n",
    "\n",
    "# Process and embed documents\n",
    "process_and_embed_documents(file_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8824056-75a2-44e1-82bc-34a218667770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries\n",
    "ask_question(\"How to create ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5a3f5-0c02-464b-b57b-58772ad7309f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c65c87-0904-4770-bd07-4029c1fd4746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
